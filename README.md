# ğŸ”¬ Blood Cell Classification using Deep Learning

**Author:** Shadman Sharif (A1944825)  
**Course:** Deep Learning Fundamentals  
**Achievement:** ğŸ† 98.0% Test Accuracy (6th Place on Leaderboard)

---

## ğŸ“Š Project Overview

Automated classification of microscopic blood cell images into 8 distinct categories using state-of-the-art deep learning techniques. This project demonstrates the application of efficient neural architectures for medical image analysis, achieving near-perfect accuracy with minimal computational resources.

### ğŸ¯ Cell Types Classified

- **Basophil** - White blood cell involved in allergic reactions
- **Eosinophil** - White blood cell fighting parasites and allergies
- **Erythroblast** - Immature red blood cell precursor
- **Immature Granulocyte (IG)** - Early stage white blood cell
- **Lymphocyte** - Key player in immune system response
- **Monocyte** - Largest white blood cell type
- **Neutrophil** - Most abundant white blood cell
- **Platelet** - Essential for blood clotting

---

## ğŸ¯ Key Results

| Metric | Value |
|--------|-------|
| **Validation Accuracy** | 99.38% |
| **Test Accuracy** | 98.0% |
| **Leaderboard Ranking** | 6th place |
| **Model Parameters** | 1.84M (93% reduction vs baseline) |
| **Training Time** | 8 minutes (50 epochs) |
| **Inference Speed** | 8.3ms per image (120 images/sec) |
| **GPU Memory** | 4.2GB peak |
| **Per-Class F1-Scores** | 98.8% - 100% |

---

## ğŸ—ï¸ Architecture: DeepEfficientNet

Custom efficient architecture based on EfficientNet with specialized enhancements for medical imaging:

### Core Components
- **Base:** EfficientNet-B0 backbone with MBConv blocks
- **Attention:** Squeeze-and-Excitation (SE) blocks for channel-wise feature recalibration
- **Regularization:** Stochastic depth, dropout (p=0.3), label smoothing (Î±=0.1)
- **Optimization:** AdamW with cosine annealing learning rate schedule
- **Training:** Mixed precision (FP16) for 1.5Ã— speedup

### Architecture Highlights
```
Input (224Ã—224Ã—3)
    â†“
Stem Conv (112Ã—112Ã—32)
    â†“
MBConv Stage 1 (112Ã—112Ã—16)
    â†“
MBConv Stage 2 (56Ã—56Ã—24)
    â†“
MBConv Stage 3 (28Ã—28Ã—40) + SE Attention
    â†“
MBConv Stage 4 (14Ã—14Ã—80) + SE Attention
    â†“
Global Average Pooling (80Ã—1)
    â†“
Dropout (p=0.3)
    â†“
Fully Connected (8 classes)
    â†“
Softmax Output
```

---

## ğŸ“ˆ Performance Improvements

Comprehensive ablation studies across 16 experiments quantified individual contributions:

| Component | Contribution | Impact |
|-----------|--------------|--------|
| **SE Attention Blocks** | +2.5% | Channel-wise feature recalibration |
| **Comprehensive Regularization** | +4.38% | Label smoothing, dropout, stochastic depth |
| **Cosine Annealing Schedule** | +3.29% | Smooth learning rate decay |
| **Enhanced Data Augmentation** | +3.6% | Rotation, flips, color jitter, affine |

### Model Comparison

| Model | Parameters | Val Accuracy | F1-Score | Improvement |
|-------|-----------|--------------|----------|-------------|
| SimpleCNN | 26.08M | 75.62% | 0.749 | Baseline |
| ResNet18 | 11.18M | 84.69% | 0.838 | +9.07% |
| EfficientNetLite | 0.55M | 96.25% | 0.953 | +20.63% |
| **DeepEfficientNet (Ours)** | **1.84M** | **99.38%** | **0.994** | **+23.76%** |

---

## ğŸ“ Academic Report

- **Format:** ICLR 2024 Conference Template
- **Length:** 6 pages (excluding references)
- **Content:**
  - Comprehensive literature review
  - Detailed methodology and architecture design
  - Systematic ablation studies (16 experiments)
  - Per-class performance analysis
  - Clinical deployment considerations
- **Figures:** 4 publication-quality visualizations
- **Tables:** 2 comprehensive results tables

ğŸ“„ **[View Full Report](report/blood_cell_classification_REORGANIZED_BEAUTIFUL.pdf)**

---

## ğŸ“ Repository Structure

```
perfect-cell-classification/
â”œâ”€â”€ ğŸ“„ README.md                          # This file
â”œâ”€â”€ ğŸ“ report/                            # Academic documentation
â”‚   â”œâ”€â”€ blood_cell_classification_REORGANIZED_BEAUTIFUL.pdf
â”‚   â”œâ”€â”€ blood_cell_report_enhanced.tex    # LaTeX source
â”‚   â””â”€â”€ FINAL_SUBMISSION_SUMMARY.txt
â”œâ”€â”€ ğŸ“ figures/                           # Visualizations
â”‚   â”œâ”€â”€ architecture.png                  # Model architecture diagram
â”‚   â”œâ”€â”€ training_curves.png               # Training/validation curves
â”‚   â”œâ”€â”€ ablation_studies.png              # Ablation study results
â”‚   â”œâ”€â”€ model_comparison_clean.png        # Baseline comparison
â”‚   â””â”€â”€ confusion_matrix.png              # Error analysis
â”œâ”€â”€ ğŸ“ code/                              # Implementation
â”‚   â”œâ”€â”€ train.py (or .ipynb)             # Training pipeline
â”‚   â”œâ”€â”€ model.py                         # Model architecture
â”‚   â”œâ”€â”€ utils.py                         # Helper functions
â”‚   â””â”€â”€ generate_figures.py              # Visualization scripts
â”œâ”€â”€ ğŸ“ data/                              # Data configurations
â”‚   â”œâ”€â”€ class_map.json                   # Class ID mapping
â”‚   â””â”€â”€ prediction_labels.json           # Test predictions
â””â”€â”€ ğŸ“ docs/                              # Additional documentation
    â”œâ”€â”€ REORGANIZATION_SUMMARY.txt
    â”œâ”€â”€ LAYOUT_COMPARISON.txt
    â””â”€â”€ publication_ready_summary.txt
```

---

## ğŸš€ Quick Start

### Prerequisites
```bash
Python 3.8+
PyTorch 2.0+
CUDA 11.8+ (for GPU training)
```

### Installation
```bash
# Clone repository
git clone https://github.com/ALFIE-SADMAN/perfect-cell-classification.git
cd perfect-cell-classification

# Install dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install numpy pandas matplotlib seaborn scikit-learn pillow
```

### Training
```python
# See code/train.py or train.ipynb for full training pipeline
python train.py --epochs 50 --batch_size 32 --lr 0.001
```

---

## ğŸ“Š Detailed Results

### Per-Class Performance

| Cell Type | Precision | Recall | F1-Score |
|-----------|-----------|--------|----------|
| Basophil | 100% | 100% | **100%** |
| Eosinophil | 100% | 98.8% | 99.4% |
| Erythroblast | 100% | 100% | **100%** |
| Immature Granulocyte | 98.8% | 100% | 99.4% |
| Lymphocyte | 100% | 98.8% | 99.4% |
| Monocyte | 98.8% | 100% | 99.4% |
| Neutrophil | 100% | 100% | **100%** |
| Platelet | 98.8% | 98.8% | 98.8% |
| **Average** | **99.5%** | **99.5%** | **99.5%** |

### Confusion Analysis

Most errors occur between morphologically similar cell types:
- **Immature Granulocytes â†” Monocytes** (biologically similar)
- **Platelet size variations** (smallest cells, challenging)

These confusion patterns are consistent with known clinical diagnostic challenges, validating the model's learned representations.

---

## ğŸ’¡ Key Technical Insights

### 1. Architecture Efficiency
- **93% parameter reduction** compared to baseline SimpleCNN
- **84% fewer parameters** than ResNet18
- Demonstrates that efficient architectures outperform traditional large models

### 2. Attention Mechanisms
- SE blocks provide **+2.5% improvement** through channel recalibration
- Most effective in deeper stages (Stages 3-4)
- Minimal computational overhead (<5% inference time)

### 3. Training Strategy
- **Label smoothing (Î±=0.1)**: Single biggest regularization contribution (+1.9%)
- **Cosine annealing**: Smooth convergence, outperforms step decay
- **Mixed precision**: 1.5Ã— speedup with no accuracy loss

### 4. Data Augmentation
- Full strategy: flips, rotation (Â±30Â°), ColorJitter, affine transforms
- **+3.6% improvement** over normalization alone
- Critical for limited dataset size (3,200 training images)

---

## ğŸ”¬ Clinical Relevance

### Potential Applications
- **Automated CBC Analysis:** Reduce manual microscopy burden
- **Quality Control:** Consistent, objective cell classification
- **High-Throughput Screening:** Process thousands of samples rapidly
- **Education:** Training tool for medical students
- **Remote Diagnostics:** Enable telemedicine in underserved areas

### Deployment Considerations
- **Inference Speed:** 8.3ms per image suitable for real-time analysis
- **Memory Footprint:** 1.84M parameters enables edge deployment
- **Calibration Needed:** Adjust for natural class imbalance in clinical settings
- **Domain Adaptation:** Fine-tune for different laboratory staining protocols

---

## ğŸ¯ Future Directions

1. **Ensemble Methods:** Combine multiple models for uncertainty quantification
2. **Self-Supervised Pre-training:** Use SimCLR/MoCo for better representations
3. **Multi-Scale Analysis:** Integrate cell context and fine morphological details
4. **Prospective Validation:** Test across multiple clinical sites
5. **Rare Cell Detection:** Extend to pathological conditions and rare cell types
6. **Explainability:** Integrate Grad-CAM for clinical interpretability

---

## ğŸ“š Technologies Used

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![CUDA](https://img.shields.io/badge/CUDA-11.8+-green.svg)

**Deep Learning:**
- PyTorch 2.0+ (Model implementation)
- torchvision (Data augmentation)
- Mixed Precision Training (AMP)

**Scientific Computing:**
- NumPy (Numerical operations)
- pandas (Data analysis)
- scikit-learn (Metrics, preprocessing)

**Visualization:**
- Matplotlib (Charts and plots)
- Seaborn (Statistical visualizations)
- Pillow (Image processing)

**Documentation:**
- LaTeX (Academic report)
- Markdown (Documentation)

---

## ğŸ† Achievements

âœ… **99.38% validation accuracy** - Near-perfect performance  
âœ… **98.0% test accuracy** - Strong generalization  
âœ… **6th place ranking** - Top performance among peers  
âœ… **93% parameter reduction** - Extreme efficiency  
âœ… **Perfect F1-scores** - 100% on 3 cell types  
âœ… **8-minute training** - Rapid experimentation  
âœ… **Publication-quality report** - Conference-standard documentation  
âœ… **16 ablation experiments** - Rigorous methodology  

---

## ğŸ“– References

Key papers that informed this work:

1. **EfficientNet:** Tan & Le, "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", ICML 2019
2. **Squeeze-and-Excitation:** Hu et al., "Squeeze-and-Excitation Networks", CVPR 2018
3. **ResNet:** He et al., "Deep Residual Learning for Image Recognition", CVPR 2016
4. **Blood Cell Analysis:** Acevedo et al., "Recognition System for Peripheral Blood Cell Images", Computer Methods and Programs in Biomedicine, 2019
5. **Medical Imaging Survey:** Litjens et al., "A Survey on Deep Learning in Medical Image Analysis", Medical Image Analysis, 2017

---

## ğŸ“§ Contact

**Shadman Sharif**  
Student ID: A1944825  
Course: Deep Learning Fundamentals  

For questions or collaboration opportunities, please open an issue or contact through GitHub.

---

## ğŸ“œ License

This project is available for educational and research purposes. Please cite this work if you use it in your research.

---

## ğŸ™ Acknowledgments

- Course instructors for guidance and feedback
- Classmates for discussions and insights
- Open-source community for excellent tools and libraries
- Dataset providers for quality medical imaging data

---

## â­ Star This Repository

If you find this project useful or interesting, please consider giving it a star! â­

---

**Last Updated:** November 2024  
**Status:** âœ… Completed | ğŸ† 6th Place Achievement | ğŸ“Š 98.0% Test Accuracy
